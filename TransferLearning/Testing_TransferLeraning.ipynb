{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intend to reuse the Alexnet thathas been created [here](https://github.com/Evolving-AI-Lab/deep_learning_for_camera_trap_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os, sys, time\n",
    "sys.path.append('/Users/manishrai/Desktop/UMN/Research/Zooniverse/deep_learning_for_camera_trap_images/phase2/architectures')\n",
    "sys.path.append('/Users/manishrai/Desktop/UMN/Research/Zooniverse/deep_learning_for_camera_trap_images/phase2/')\n",
    "sys.path.append('/Users/manishrai/Desktop/UMN/Research/Zooniverse/Code/Data_gathering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ImageToArray, cnn_utils\n",
    "import common, alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 55.6005470753 seconds to calculate.\n",
      "memory usage for training data - 690487296\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "# directory of the training dataset\n",
    "train_data_dog_dir = '/Users/manishrai/Desktop/UMN/Research/Zooniverse/Data/CatsVsDogs_test/train/dogs/'\n",
    "train_data_cat_dir = '/Users/manishrai/Desktop/UMN/Research/Zooniverse/Data/CatsVsDogs_test/train/cats/'\n",
    "\n",
    "X_train_dogs = ImageToArray.get_image_to_array(train_data_dog_dir, w = 128, h = 128)\n",
    "X_train_cats = ImageToArray.get_image_to_array(train_data_cat_dir, w = 128, h = 128)\n",
    "Y_train_dogs = np.ones((1, X_train_dogs.shape[0]), dtype=np.int8)\n",
    "Y_train_cats = np.zeros((1, X_train_cats.shape[0]), dtype=np.int8)\n",
    "\n",
    "\n",
    "# Append and permuate X and Y\n",
    "X_train_orig =  np.concatenate((X_train_dogs, X_train_cats), axis = 0 )\n",
    "m = X_train_orig.shape[0] # number of training examples\n",
    "Y_train_orig =  np.concatenate((Y_train_dogs, Y_train_cats), axis = 1 ).reshape(m, 1)\n",
    "\n",
    "# Randomly shuffle the arrays\n",
    "shuffle_index_train = np.random.permutation(range(m))\n",
    "X_train_orig_p = X_train_orig[shuffle_index_train]\n",
    "Y_train_orig_p = Y_train_orig[shuffle_index_train]\n",
    "\n",
    "# Normalize the image arrays\n",
    "X_train_orig_norm = ImageToArray.get_normalized_pixels(X_train_orig_p)\n",
    "endTime = time.time()\n",
    "\n",
    "\n",
    "print('Took %s seconds to calculate.' % (endTime - startTime))\n",
    "print('memory usage for training data - %s' % X_train_dogs.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(X_train_orig_p, dtype = \"float32\")\n",
    "Y_train = tf.convert_to_tensor(Y_train_orig_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<bound method Tensor.get_shape of <tf.Tensor 'Const:0' shape=(3560, 128, 128, 3) dtype=float32>>,\n",
       " <bound method Tensor.get_shape of <tf.Tensor 'Const_1:0' shape=(3560, 1) dtype=int8>>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.get_shape, Y_train.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const:0\", shape=(3560, 128, 128, 3), dtype=float32) must be from the same graph as Tensor(\"conv1/weights:0\", shape=(11, 11, 3, 64), dtype=float32_ref, device=/device:CPU:0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cff9961f84c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_mode\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/manishrai/Desktop/UMN/Research/Zooniverse/deep_learning_for_camera_trap_images/phase2/architectures/alexnet.pyc\u001b[0m in \u001b[0;36minference\u001b[0;34m(x, num_output, wd, dropout_rate, is_training, transfer_mode)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatialConvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/manishrai/Desktop/UMN/Research/Zooniverse/deep_learning_for_camera_trap_images/phase2/architectures/common.pyc\u001b[0m in \u001b[0;36mspatialConvolution\u001b[0;34m(x, ksize, stride, filters_out, wd, weight_initializer, bias_initializer)\u001b[0m\n\u001b[1;32m    105\u001b[0m                             shape, weight_initializer, tf.contrib.layers.l2_regularizer(wd))\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'biases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilters_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbias_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    397\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m       \u001b[0;31m# pyline: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   3898\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3899\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3900\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3901\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3902\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   3837\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3838\u001b[0m     raise ValueError(\n\u001b[0;32m-> 3839\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   3840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const:0\", shape=(3560, 128, 128, 3), dtype=float32) must be from the same graph as Tensor(\"conv1/weights:0\", shape=(11, 11, 3, 64), dtype=float32_ref, device=/device:CPU:0)."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with tf.Session() as sess:\n",
    "    with g.as_default():\n",
    "        alexnet.inference(X_train, 2, 0.00, 0.5, None, transfer_mode= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "    \"\"\" Resize Image \"\"\"\n",
    "    image = tf.image.resize_images(image, size=target_size)\n",
    "    image = tf.divide(image, 255.0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = '/Users/manishrai/Desktop/UMN/Research/Zooniverse/Data/CatsVsDogs_test/train/dogs/dog.1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "image = resize_image(X_train_dogs[1], [224, 224]) # The image data has to be in an array format\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _crop(image, offset_height, offset_width, crop_height, crop_width):\n",
    "    \"\"\"Crops the given image using the provided offsets and sizes.\n",
    "    Note that the method doesn't assume we know the input image size but it does\n",
    "    assume we know the input image rank.\n",
    "    Args:\n",
    "    image: an image of shape [height, width, channels].\n",
    "    offset_height: a scalar tensor indicating the height offset.\n",
    "    offset_width: a scalar tensor indicating the width offset.\n",
    "    crop_height: the height of the cropped image.\n",
    "    crop_width: the width of the cropped image.\n",
    "    Returns:\n",
    "    the cropped (and resized) image.\n",
    "    Raises:\n",
    "    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n",
    "      less than the crop size.\n",
    "    \"\"\"\n",
    "    original_shape = tf.shape(image)\n",
    "\n",
    "    rank_assertion = tf.Assert(\n",
    "      tf.equal(tf.rank(image), 3),\n",
    "      ['Rank of image must be equal to 3.'])\n",
    "    with tf.control_dependencies([rank_assertion]):\n",
    "        cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n",
    "\n",
    "    size_assertion = tf.Assert(\n",
    "        tf.logical_and(\n",
    "          tf.greater_equal(original_shape[0], crop_height),\n",
    "          tf.greater_equal(original_shape[1], crop_width)),\n",
    "        ['Crop size greater than the image size.'])\n",
    "\n",
    "    offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n",
    "\n",
    "    # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n",
    "    # define the crop size.\n",
    "    with tf.control_dependencies([size_assertion]):\n",
    "        image = tf.slice(image, offsets, cropped_shape)\n",
    "    return tf.reshape(image, cropped_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.63017124,  0.54053545,  0.41208398],\n",
       "        [ 0.61864728,  0.52901137,  0.39991963],\n",
       "        [ 0.5823127 ,  0.49235675,  0.36374524],\n",
       "        ..., \n",
       "        [ 0.62000805,  0.38303334,  0.1611046 ],\n",
       "        [ 0.63377327,  0.3973588 ,  0.17238878],\n",
       "        [ 0.63217258,  0.39799902,  0.16278481]],\n",
       "\n",
       "       [[ 0.50212103,  0.41472608,  0.27442992],\n",
       "        [ 0.57895166,  0.4915567 ,  0.34805927],\n",
       "        [ 0.59895945,  0.50996381,  0.3688674 ],\n",
       "        ..., \n",
       "        [ 0.60144049,  0.3734293 ,  0.15790308],\n",
       "        [ 0.59631842,  0.36886746,  0.14933962],\n",
       "        [ 0.58831513,  0.36310509,  0.13717464]],\n",
       "\n",
       "       [[ 0.46402639,  0.37791196,  0.22721158],\n",
       "        [ 0.55430186,  0.46882766,  0.31364554],\n",
       "        [ 0.59383738,  0.50548196,  0.35462168],\n",
       "        ..., \n",
       "        [ 0.59839934,  0.38335338,  0.17038815],\n",
       "        [ 0.58783519,  0.37366962,  0.15574239],\n",
       "        [ 0.57727087,  0.36470595,  0.14229688]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.63353324,  0.42016786,  0.21008389],\n",
       "        [ 0.6085633 ,  0.39103624,  0.18767492],\n",
       "        [ 0.60120046,  0.38591442,  0.18207282],\n",
       "        ..., \n",
       "        [ 0.83697665,  0.76358801,  0.65906698],\n",
       "        [ 0.7843948 ,  0.7137267 ,  0.61280721],\n",
       "        [ 0.63457191,  0.52772886,  0.44217581]],\n",
       "\n",
       "       [[ 0.6320926 ,  0.4153659 ,  0.20688258],\n",
       "        [ 0.60360152,  0.38495398,  0.18287319],\n",
       "        [ 0.60168087,  0.38543445,  0.1828734 ],\n",
       "        ..., \n",
       "        [ 0.89571548,  0.86482322,  0.81680399],\n",
       "        [ 0.81128061,  0.77638656,  0.74181283],\n",
       "        [ 0.61664259,  0.5407719 ,  0.52252471]],\n",
       "\n",
       "       [[ 0.62152833,  0.40288091,  0.20080021],\n",
       "        [ 0.61352557,  0.39423785,  0.19727913],\n",
       "        [ 0.62184918,  0.40560269,  0.20624295],\n",
       "        ..., \n",
       "        [ 0.47858277,  0.45633432,  0.38654652],\n",
       "        [ 0.32532331,  0.32116196,  0.28498745],\n",
       "        [ 0.42776981,  0.39799818,  0.36278388]]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(_crop(image, 100, 100, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
